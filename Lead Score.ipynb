{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Statement\n\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\n \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\n \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. \n\nData\nYou have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column ‘Converted’ which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn’t converted. You can learn more about the dataset from the data dictionary provided in the zip folder at the end of the page. Another thing that you also need to check out for are the levels present in the categorical variables. Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value (think why?)."},{"metadata":{},"cell_type":"markdown","source":"# Objective\nBuilt a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\n\nThere are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in your final PPT where you'll make recommendations."},{"metadata":{},"cell_type":"markdown","source":"# Problem Solving Methodology\n\n### Step 1. Load the dataset\n### Step 2. Data Cleaning & Preparation\n### Step 3. Treatment of Null Values\n### Step 4. Outliers Detection & Treatment\n### Step 5. Univariate Exploratory Data Analysis\n### Step 6. Model Building & Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the required libraries\nimport numpy  as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import xticks\nimport seaborn as sns\n### Supress unnecessary warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Load the DataSet"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the dataset\nleads = pd.read_csv(\"../input/leadds/Leads.csv\")\nleads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the dimensions of the datset\nleads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the dataset\nleads.info()\n\n# Observation: there are null values present in the dataset, we will treat these nulls later","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check for duplicates\nleads[leads[\"Prospect ID\"].duplicated()==True]\n\n# no duplicates found","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Cleaning & Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check % of null values for each column\nround(100*(leads.isnull().sum()/len(leads.index)), 2).sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets drop columns with null values greater than 30%\ndrop_cols = []\nfor col in leads.columns:\n    if round(100*(leads[col].isnull().sum()/len(leads.index)), 2)>30:\n        drop_cols.append(col)\n\nprint(\"dropping columns:\",drop_cols)\nleads.drop(drop_cols,1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check % of null values again\nround(100*(leads.isnull().sum()/len(leads.index)), 2).sort_values(ascending = False)\n\n# we will treat these null values in a while","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check unique valeus for all the variables except \"Prospect ID\" & \" Lead Number\"\ncols = leads.drop([\"Prospect ID\",\"Lead Number\"],axis = 1)\n\nprint('\\nUnique values in the dataframe - column wise:')\nfor i in cols:\n    print(i,leads[i].unique(),'\\n')\n    \n# we can see columns having \"Select\" values, which means user did not select any value for these columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets get the list of columns having \"Select\" values\nprint(\"Column name containing value Select:\\n\")\nfor col in cols:\n    for value in leads[col].values:\n        if value == \"Select\":\n            print(col)\n            break\n# lets check columns with \"Select\" values one by one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check \"Specialization\" for \"Select\"\nprint(\"Count of different values for Specialization:\",leads[\"Specialization\"].value_counts())\n\n# There are 1942 records present in the dataset with Specialization value as \"Select\", we will drop these records\nprint(\"\\nDropping the rows with value Select\" )\nleads = leads[~(leads['Specialization'] == \"Select\")]\n      \nprint(\"\\nCount of final values for Specialization:\",leads[\"Specialization\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check \"How did you hear about X Education\" for \"Select\"\nprint(\"How did you hear about x education values:\\n\",leads[\"How did you hear about X Education\"].value_counts())\n# majority of the records have \"Select\" for this fields, lets drop the column\n\nprint(\"\\nDropping the field as it has too many rows with Select Value\")\nleads = leads.drop(\"How did you hear about X Education\",1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check \"Lead Profile\" for \"Select\"\n\nprint(\"Lead Profile Values:\",leads[\"Lead Profile\"].value_counts())\nprint(\"\\nDropping the field as it has too many rows with Select Value\")\nleads = leads.drop(\"Lead Profile\",1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check \"City\" for \"Select\"\nprint(\"City values:\",leads[\"City\"].value_counts())\n\n# majority of the records have only 2 values for this field \"Mumbai\" & \"Select\", we can drop this column\nprint(\"\\nDropping the field as it has too many rows with Select Value\")\nleads = leads.drop(\"City\",1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are done treating \"Select\" Values, now lets correct other values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# field Lead Source has values like google & Google, lets make it \"google\"\nleads[\"Lead Source\"] = leads[\"Lead Source\"].replace(\"Google\",\"google\")\n\n# lets check whether the value has been corrected or not\nleads[\"Lead Source\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have made all the corrections now, lets check for variable with single values or almost no variation and drop such variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the variations for each column\n\nfor col in leads.drop([\"Prospect ID\",\"Converted\",\"Lead Number\"],1).columns:\n    print(\"column name:\",col,\"\\n\",leads[col].value_counts(),\"\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Based on above stats we can we can drop following features as these features do not have much variation\n1. Do Not Call\n2. Search\n3. Magazine\n4. Newspaper Article\n5. X Education Forums\n6. Newspaper\n7. Digital Advertisement\n8. Through Recommendations\n9. Receive More Updates About Our Courses \n10. Update me on Supply Chain Content\n11. Get updates on DM Content\n12. I agree to pay the amount through cheque\n13. What matters most to you in choosing a course \n14. Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets drop all the columns mentioned above\n\nleads.drop(['Do Not Call', 'Country','Search', 'Magazine', 'Newspaper Article', 'X Education Forums', 'Newspaper', \n            'Digital Advertisement', 'Through Recommendations', 'Receive More Updates About Our Courses', \n            'Update me on Supply Chain Content', 'Get updates on DM Content', \n            'I agree to pay the amount through cheque','What matters most to you in choosing a course'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Treat Null Values\n\n##### Lets treat null values now"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for null values\nleads.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check % of null values for each column\nround(100*(leads.isnull().sum()/len(leads.index)), 2).sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check What is your current occupation field\n\nprint(\"\\nNo. of Null Values:\",leads[\"What is your current occupation\"].isnull().sum())\nprint(\"Check different values:\\n\",leads[\"What is your current occupation\"].value_counts())\n\n# lets replace the null values with the most frequent value\nleads[\"What is your current occupation\"] = leads[\"What is your current occupation\"].replace(np.nan,\"Unemployed\")\n\nprint(\"\\nNo. of Null Values after treating null values:\",leads[\"What is your current occupation\"].isnull().sum())\nprint(\"Check different values:\\n\",leads[\"What is your current occupation\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check What is your current occupation field\n\nprint(\"\\nNo. of Null Values:\",leads[\"Specialization\"].isnull().sum())\nprint(\"Check different values:\\n\",leads[\"Specialization\"].value_counts())\n\n# we will remove the rows with null values\nleads = leads[~pd.isnull(leads['Specialization'])]\n\nprint(\"\\nNo. of Null Values after treating null values:\",leads[\"Specialization\"].isnull().sum())\nprint(\"Check different values:\\n\",leads[\"Specialization\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check Page Views Per Visit field\nprint(\"\\nNo. of Null Values for Page Views Per Visit:\",leads[\"Page Views Per Visit\"].isnull().sum())\nprint(\"Describe Page Views Per Visit:\",leads[\"Page Views Per Visit\"].value_counts())\n\n# we will remove the records with null values\nleads = leads[~pd.isnull(leads['Page Views Per Visit'])]\n\nprint(\"\\nNo. of Null Values for Page Views Per Visit after treatment:\",leads[\"Page Views Per Visit\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check TotalVisits field\nprint(\"\\nNo. of Null Values:\",leads[\"TotalVisits\"].isnull().sum())\n# no null values left","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check Values for Page Views Per Visit field\nprint(\"\\nNo. of Null Values for Last Activity:\",leads[\"Last Activity\"].isnull().sum())\n# no null values left","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the column 'Lead Source'\nprint(\"\\nNo. of Null Values for Lead Source:\",leads[\"Lead Source\"].isnull().sum())\nprint(\"Describe Lead Source:\",leads[\"Lead Source\"].value_counts())\n\n# we will remove the rows with null values\nleads = leads[~pd.isnull(leads['Lead Source'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check % of null values again\nround(100*(leads.isnull().sum()/len(leads.index)), 2).sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have treated all the null values"},{"metadata":{},"cell_type":"markdown","source":"# 4. Outliers Detection & Treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets drop \"Prospect ID\" and \"Lead Number\" because are of no use in our analysis\nleads = leads.drop([\"Prospect ID\", \"Lead Number\"],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.info()\n\n# lets check numeric fields TotalVisits, Total Time Spent on Website,Page Views Per Visit for outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_treatment(data ,field):\n    plt.figure(figsize=(10,8))\n    plt.subplot(1,2,1)\n    plt.boxplot(data[field])\n    Q1 = data[field].quantile(0.05)\n    Q3 = data[field].quantile(0.95)\n    IQR = Q3 - Q1\n    data = data[(data[field]>= Q1) & (data[field] <= Q3)]\n    plt.title(\"Before Outlier Treatment\")\n    \n    plt.subplot(1,2,2)\n    plt.boxplot(data[field])\n    plt.title(\"After Outlier Treatment\")\n    return(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads = outlier_treatment(leads,\"TotalVisits\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads = outlier_treatment(leads,\"Total Time Spent on Website\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads = outlier_treatment(leads,\"Page Views Per Visit\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have succesfully treated the outliers, lets perform EDA now"},{"metadata":{},"cell_type":"markdown","source":"# 5. Univariate Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets take a copy of the dataset to perform eda, we are taking a copy because we would create \"binning\" variables \n# for the fields having more than 30 unique values\nleads_eda = pd.DataFrame(leads).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_eda.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for univariate analysis\n# if an integer variable has more than 30 unique values, we will create bins for the variable\ndef univariate_plot(data,col):            \n    if data[col].nunique() > 30:\n        col_bins = col+\"_bins\"\n        data[col_bins] = pd.cut(data[col], 8, duplicates = 'drop') # creating bins                                  \n        sns.countplot(data[col_bins]) # plot for binned variables\n        plt.xlabel(col_bins,fontsize = 15)\n    else:        \n        sns.countplot(data[col]) # plot for non binned variables\n        plt.xlabel(col,fontsize = 15)\n    \n    plt.ylabel('Frequency',fontsize = 15)\n    xticks(rotation = 30)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets perform univariate analysis for numeric type variables\nplt.figure(figsize=(25,30))\nfin = []\ncols = leads_eda.columns\nfor col in cols:\n    if leads_eda[col].dtypes != 'O': # getting the list of numeric variables\n        fin.append(col)\n        \nfor idx,col in enumerate(fin): # plotting for numeric variables\n    plt.subplot(3, 2, idx+1)\n    univariate_plot(leads_eda,col)          ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation from above plots:\n1. Converted: column has good reprsentation for both the possible values\n2. Total Time Spent onf website: most of the visitors spent less than 402 seconds on the website"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets perform univariate analysis for non numeric variables\nfin = []\ncols = leads_eda.columns\nfor col in cols:\n    if leads_eda[col].dtypes == 'O': # getting the list of non numeric variables\n        fin.append(col)\n\nplt.figure(figsize=(25,60)) # plotting graphs for non numeric variables\nfor idx,col in enumerate(fin):            \n    plt.subplot(4, 2, idx+1)\n    sns.countplot(leads_eda[col])\n    plt.title(\"Count Plot for \"+ col)\n    xticks(rotation = 30)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations from above graphs:\n1. Lead Origin : majority of the leads originate from  \"Lending Page Submission\"\n2. Direct Traffic & google are few of the top Lead Sources\n3. Do not Email: Majority of the leads seems to have selected NO\n4. Last Activity : for majority of the leads, last activity is either \"Email Opened\" or \"SMS Sent\"\n5. Specialization: data is evenly distributed for this column\n6. What is your current occupation: majority of the leads are unemployed.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(leads.index))\nprint(len(leads.index)/9240)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are left with around 54% of the original dataset now"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check whether the dataset is balanced or not\nlen(leads[leads[\"Converted\"]==1])/len(leads.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have balanced dataset."},{"metadata":{},"cell_type":"markdown","source":"# Dummy Variable Creation"},{"metadata":{},"cell_type":"markdown","source":"Lets create dummy variables for the categorical features present in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets find out categorical variables\nobj = [col        for col in leads.columns     if leads[col].dtype == \"O\"]\nobj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dummy variables (except Prospect ID and ) using the 'get_dummies' command\ndummy = pd.get_dummies(leads[['Lead Origin', 'Lead Source', 'Do Not Email', 'Last Activity',\n                              'What is your current occupation','A free copy of Mastering The Interview', \n                              'Last Notable Activity','Specialization']], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets drop the variables for which we have created the dummy variables above\nleads = leads.drop(['Lead Origin', 'Lead Source', 'Do Not Email', 'Last Activity',\n                              'What is your current occupation','A free copy of Mastering The Interview', \n                              'Last Notable Activity','Specialization'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads = pd.concat([leads,dummy],1)\nleads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the required library\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put all the feature variables in X\n\nX = leads.drop(['Converted'], 1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put the target variable in y\n\ny = leads['Converted']\n\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the dataset into 70% train and 30% test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import MinMax scaler\n\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the three numeric features present in the dataset\n\nscaler = MinMaxScaler()\n\nX_train[['TotalVisits', 'Page Views Per Visit', 'Total Time Spent on Website']] = scaler.fit_transform(X_train[['TotalVisits', 'Page Views Per Visit', 'Total Time Spent on Website']])\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import 'LogisticRegression' and create a LogisticRegression object\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import RFE and select 15 variables\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at which features have been selected by RFE\n\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put all the columns selected by RFE in the variable 'col'\n\ncol = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select only the columns selected by RFE\n\nX_train = X_train[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import statsmodels\n\nimport statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a logistic Regression model on X_train after adding a constant and output the summary\n\nX_train_sm = sm.add_constant(X_train)\nlogm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    # Import 'variance_inflation_factor'\n\n    from statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# p-value & VIF for Lead Origin_Lead Add Form is high, lets drop this column\nX_train.drop('Lead Origin_Lead Add Form', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refit the model with the new set of features\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# p-value for this field is too high, lets drop\nX_train.drop('Lead Source_Welingak Website', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refit the model with the new set of features\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# p-value for this field is too high, lets drop\nX_train.drop('What is your current occupation_Housewife', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refit the model with the new set of features\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nres = logm1.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VIF for this field is too high, lets drop\nX_train.drop('What is your current occupation_Unemployed', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refit the model with the new set of features\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nres = logm1.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# p-value for this field is too high, lets drop\nX_train.drop('What is your current occupation_Other', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refit the model with the new set of features\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nres = logm1.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have finally got a set of features with p-values & VIF in permissible limit , now lets evaluate the model"},{"metadata":{},"cell_type":"markdown","source":"# 7. Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use 'predict' to predict the probabilities on the train set\n\ny_train_pred = res.predict(sm.add_constant(X_train))\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new dataframe containing the actual conversion flag and the probabilities predicted by the model\n\ny_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import metrics from sklearn for evaluation\n\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create confusion matrix \n\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted )\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy\n\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's evaluate the other metrics as wella\n\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the sensitivity\n\nTP/(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the specificity\n\nTN/(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC function\n\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob, drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import matplotlib to plot the ROC curve\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Call the ROC function\n\ndraw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \n\nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a dataframe to see the values of accuracy, sensitivity, and specificity at different values of probabiity cutoffs\n\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot it as well\n\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have intersection at around cut off value 0f 0.4"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.4 else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the accuracy now\n\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create the confusion matrix once again\n\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's evaluate the other metrics as well\n\nTP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Sensitivity\n\nTP/(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Let's now make predicitons on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the three numeric features present in the dataset\n\nscaler = MinMaxScaler()\n\nX_test[['Total Time Spent on Website','Page Views Per Visit','TotalVisits']] = scaler.fit_transform(X_test[['Total Time Spent on Website','Page Views Per Visit','TotalVisits']])\n\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the columns in X_train for X_test as well\n\nX_test = X_test[col]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant to X_test\n\nX_test_sm = sm.add_constant(X_test[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the required columns from X_test as well\n\nX_test.drop(['Lead Origin_Lead Add Form','Lead Source_Welingak Website','What is your current occupation_Housewife',\n             'What is your current occupation_Unemployed','What is your current occupation_Other'], 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on the test set and store it in the variable 'y_test_pred'\n\ny_test_pred = res.predict(sm.add_constant(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_pred to a dataframe\n\ny_pred_1 = pd.DataFrame(y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head\n\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\n\ny_test_df = pd.DataFrame(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove index for both dataframes to append them side by side \n\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Append y_test_df and y_pred_1\n\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check 'y_pred_final'\n\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename the column \n\ny_pred_final= y_pred_final.rename(columns = {0 : 'Conversion_Prob'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head of y_pred_final\n\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on the test set using 0.4 as the cutoff\n\ny_pred_final['final_predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.4 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check y_pred_final\n\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy\n\nmetrics.accuracy_score(y_pred_final['Converted'], y_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_pred_final['Converted'], y_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate sensitivity\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision Recall View"},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted )\nconfusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision\nconfusion[1,1]/(confusion[0,1]+confusion[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recall\n\nconfusion[1,1]/(confusion[1,0]+confusion[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final.Converted, y_train_pred_final.Predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.4 else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    # Let's check the accuracy now\n\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create the confusion matrix once again\n\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's evaluate the other metrics as well\n\nTP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Precision\n\nTP/(TP+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Recall\n\nTP/(TP+FN)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}